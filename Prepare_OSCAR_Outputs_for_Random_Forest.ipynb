{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prepare_OSCAR_Outputs_for_Random_Forest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LBBFqbAsvsn"
      },
      "source": [
        "##Pytorch Transformer Initialization Tutorial \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llOPoZNCEj_I"
      },
      "source": [
        "# !pip install pytorch-pretrained-bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS5UL1ErErLg"
      },
      "source": [
        "# from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0acMf8_IEtdq"
      },
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
        "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koog2TmtEuBA"
      },
      "source": [
        "# import torch\n",
        "# output_model_file = \"my_own_model_file.bin\"\n",
        "# output_config_file = \"my_own_config_file.json\"\n",
        "# output_vocab_file = \"./\"\n",
        "\n",
        "# # Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
        "\n",
        "# # If we have a distributed model, save only the encapsulated model\n",
        "# # (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
        "# model_to_save = model.module if hasattr(model, 'module') else model\n",
        "\n",
        "# torch.save(model_to_save.state_dict(), output_model_file)\n",
        "# model_to_save.config.to_json_file(output_config_file)\n",
        "# tokenizer.save_vocabulary(output_vocab_file)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAWulnO7E8RU"
      },
      "source": [
        "# from pytorch_pretrained_bert import BertConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEzmJmX0E-Ma"
      },
      "source": [
        "# config = BertConfig.from_json_file('my_own_config_file.json')\n",
        "# model = BertForSequenceClassification(config, num_labels=2)\n",
        "# state_dict = torch.load('my_own_model_file.bin')\n",
        "# model.load_state_dict(state_dict)\n",
        "# tokenizer = BertTokenizer('vocab.txt', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU_7qZ8YKWUU"
      },
      "source": [
        "# !mv my_own_config_file.json config.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f07edniJKX-M"
      },
      "source": [
        "# !mv my_own_model_file.bin pytorch_model.bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEZ8RFWrKZuV"
      },
      "source": [
        "# model1 = BertForNextSentencePrediction.from_pretrained('./')\n",
        "# tokenizer1 = BertTokenizer.from_pretrained('./')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GDg0uZ8s-8p"
      },
      "source": [
        "##installing OSCAR "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRQcgf5MtVDP",
        "outputId": "9ee7fd37-4fc5-4a23-acbb-14998f030989"
      },
      "source": [
        "#%cd /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SwnLClu5xhg"
      },
      "source": [
        "#!rm -r /content/drive/MyDrive/Oscar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbjlzUbBovK0"
      },
      "source": [
        "#!rm -r /content/drive/MyDrive/pytorch_transformers.egg-info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZchcC67-bWx"
      },
      "source": [
        "#!rm -r 067923d3267325f525f4e46f357360c191ba562e.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQFvmLOa-jaO"
      },
      "source": [
        "#!rm -r transformers-067923d3267325f525f4e46f357360c191ba562e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75Yni2U3tomB"
      },
      "source": [
        "#!git clone https://github.com/microsoft/Oscar.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmjazO7-uwPy"
      },
      "source": [
        "%cd /content/drive/MyDrive/Oscar\n",
        "\n",
        "!python setup.py build develop\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2ny26Xv9G9R"
      },
      "source": [
        "from oscar.modeling.modeling_bert import BertImgForPreTraining"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkUgA5Iu1Z6M"
      },
      "source": [
        " #%cd /content/drive/MyDrive/Oscar/transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWdD2-NK6Ior"
      },
      "source": [
        "#!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me0zhG7h42bP"
      },
      "source": [
        "if this directory is empty, go to https://github.com/huggingface/transformers/tree/067923d3267325f525f4e46f357360c191ba562e and dowload a zip version using wget, make sure to move the unzipped files to this directory\n",
        "\n",
        "The zip download link is https://github.com/huggingface/transformers/archive/067923d3267325f525f4e46f357360c191ba562e.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNJHYJr56Llw"
      },
      "source": [
        "#%cd /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlR2KAz06jTS"
      },
      "source": [
        "#!wget https://github.com/huggingface/transformers/archive/067923d3267325f525f4e46f357360c191ba562e.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7cNMGtO6lln"
      },
      "source": [
        "#!unzip 067923d3267325f525f4e46f357360c191ba562e.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjnFc7CW7Tr6"
      },
      "source": [
        "#!mv /content/drive/MyDrive/transformers-067923d3267325f525f4e46f357360c191ba562e/* /content/drive/MyDrive/Oscar/transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC56EG5l7c4R"
      },
      "source": [
        "#!ls /content/drive/MyDrive/Oscar/transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS76-jf27jII"
      },
      "source": [
        "#!cd /content/drive/MyDrive/Oscar/transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrSRTwGFzxK5"
      },
      "source": [
        "#!python3 /content/drive/MyDrive/Oscar/transformers/setup.py build develop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW03E6iG-zND"
      },
      "source": [
        "if FileNotFoundError: [Errno 2] No such file or directory: 'README.md', go to 'setup.py' and modify line 45:\n",
        "\n",
        "\n",
        "change \"README.md\" to \"/content/drive/MyDrive/Oscar/transformers/README.md\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7vSaPAy197K"
      },
      "source": [
        "#!pip install -r /content/drive/MyDrive/Oscar/transformers/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V3Oo8rwGiZx"
      },
      "source": [
        "##Vanilla OSCAR with classifier, whose calss is **BertImgForPreTraining**\n",
        "##Vanilla OSCAR is stored in its **bert** attribute, whose class is  **BertImgModel**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvn_LtMZgUUR"
      },
      "source": [
        "# !cd /content/drive/MyDrive\n",
        "\n",
        "# !wget https://biglmdiag.blob.core.windows.net/oscar/pretrained_models/base-vg-labels.zip\n",
        "\n",
        "# !unzip /content/drive/MyDrive/base-vg-labels.zip\n",
        "\n",
        "# !mv /content/drive/MyDrive/base-vg-labels/vg-oscar /content/drive/MyDrive/2021_Hate_Speech_Detection/model_pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap4ISDLo29QW"
      },
      "source": [
        "# from oscar.modeling.modeling_bert import BertImgForPreTraining\n",
        "# from transformers.pytorch_transformers import BertConfig\n",
        "\n",
        "# config = BertConfig.from_json_file('/content/drive/MyDrive/2021_Hate_Speech_Detection/model_pth/vg-oscar/config.json')\n",
        "# model1 =   BertImgForPreTraining(config)\n",
        "# state_dict = torch.load('/content/drive/MyDrive/2021_Hate_Speech_Detection/model_pth/vg-oscar/pytorch_model.bin', map_location=torch.device('cpu'))\n",
        "\n",
        "# model1.load_state_dict(state_dict)\n",
        "# model.eval()\n",
        "# oscar_state_dict = model.bert.state_dict()\n",
        "# torch.save(oscar_state_dict, oscar_state_dict_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctwPITtddJFI"
      },
      "source": [
        "##Load objects and features from VinVl\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrElWdNpNl71"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import os.path as op\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "def load_labelmap_file(labelmap_file):\n",
        "    label_dict = None\n",
        "\n",
        "    if labelmap_file.endswith('json'):\n",
        "        label_dict = json.load(open(labelmap_file, 'r'))['label_to_idx']\n",
        "        label_dict = {key:val-1 for key, val in label_dict.items()}\n",
        "        return label_dict\n",
        "\n",
        "    if labelmap_file is not None and op.isfile(labelmap_file):\n",
        "        label_dict = OrderedDict()\n",
        "        with open(labelmap_file, 'r') as fp:\n",
        "            for line in fp:\n",
        "                label = line.strip().split('\\t')[0]\n",
        "                if label in label_dict:\n",
        "                    raise ValueError(\"Duplicate label \" + label + \" in labelmap.\")\n",
        "                else:\n",
        "                    label_dict[label] = len(label_dict)\n",
        "    return label_dict\n",
        "\n",
        "\n",
        "#return dictionary of image names and corresponidng objects names strings\n",
        "def get_objects(vinvl_prediction_dictionary, dataset_labelmap):\n",
        "  #get objects from each vinvl prediction, and concatenate them to string\n",
        "  objects_strings={}\n",
        "  for img_name in list(vinvl_prediction_dictionary.keys()):\n",
        "    object_string = ''\n",
        "    for i in vinvl_prediction_dictionary[img_name]['labels']:\n",
        "      box_feature = i\n",
        "      object_string+=dataset_labelmap[i.item()]+' '\n",
        "\n",
        "    objects_strings[img_name] = object_string\n",
        "  return objects_strings\n",
        "\n",
        "#get a dictionary of 2-D image feature tensors (n, 2054), n is the number of objects in an image\n",
        "def get_img_features(vinvl_prediction_dictionary):\n",
        "  feature_dict = {}\n",
        "  for img_name in list(vinvl_prediction_dictionary.keys()):\n",
        "    feature_list = []\n",
        "    for a, b in zip(vinvl_prediction_dictionary[img_name]['box_features'], vinvl_prediction_dictionary[img_name]['spatial_features']):\n",
        "      object_feature = torch.cat((a, b))\n",
        "      feature_list.append(object_feature)\n",
        "    \n",
        "    feature_dict[img_name] = torch.stack(feature_list)\n",
        "\n",
        "  return feature_dict\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zklqvnYzr-dM"
      },
      "source": [
        "#args.max_seq_length = 128\n",
        "def tensorize_example(tokenizer, text_a, text_b, img_feat,  cls_token_at_end=False, pad_on_left=False,\n",
        "                cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
        "                sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "                mask_padding_with_zero=True):\n",
        "    max_seq_length = 128\n",
        "    max_img_seq_length = 40\n",
        "    tokens_a = tokenizer.tokenize(text_a)\n",
        "\n",
        "    tokens_b = None\n",
        "    if text_b:\n",
        "        tokens_b = tokenizer.tokenize(text_b)\n",
        "        # Modifies `tokens_a` and `tokens_b` in place so that the total length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "    else:\n",
        "        # Account for [CLS] and     [SEP] with \"- 2\"\n",
        "        if len(tokens_a) > max_seq_length - 2:\n",
        "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "    tokens = tokens_a + [sep_token]\n",
        "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "    if tokens_b:\n",
        "        tokens += tokens_b + [sep_token]\n",
        "        segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "    if cls_token_at_end:\n",
        "        tokens = tokens + [cls_token]\n",
        "        segment_ids = segment_ids + [cls_token_segment_id]\n",
        "    else:\n",
        "        tokens = [cls_token] + tokens\n",
        "        segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    padding_length = max_seq_length - len(input_ids)\n",
        "    if pad_on_left:\n",
        "        input_ids = ([pad_token] * padding_length) + input_ids\n",
        "        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
        "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
        "    else:\n",
        "        input_ids = input_ids + ([pad_token] * padding_length)\n",
        "        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "\n",
        "    # image features\n",
        "    \n",
        "\n",
        "    if img_feat.shape[0] > max_img_seq_length:\n",
        "        img_feat = img_feat[0: max_img_seq_length, ]\n",
        "        if max_img_seq_length > 0:\n",
        "            input_mask = input_mask + [1 if mask_padding_with_zero else 0] * img_feat.shape[0]\n",
        "            segment_ids += [sequence_b_segment_id] * img_feat.shape[0]\n",
        "    else:\n",
        "        if max_img_seq_length > 0:\n",
        "            input_mask = input_mask + [1 if mask_padding_with_zero else 0] * img_feat.shape[0]\n",
        "            segment_ids = segment_ids + [sequence_b_segment_id] * img_feat.shape[0]\n",
        "        padding_matrix = torch.zeros((max_img_seq_length - img_feat.shape[0], img_feat.shape[1]))\n",
        "        img_feat = torch.cat((img_feat, padding_matrix), 0)\n",
        "        if max_img_seq_length > 0:\n",
        "            input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_matrix.shape[0])\n",
        "            segment_ids = segment_ids + [pad_token_segment_id] * padding_matrix.shape[0]\n",
        "\n",
        " \n",
        "\n",
        "    return (torch.tensor(input_ids, dtype=torch.long).unsqueeze(0),\n",
        "            torch.tensor(input_mask, dtype=torch.long).unsqueeze(0),\n",
        "            torch.tensor(segment_ids, dtype=torch.long)[0:128].unsqueeze(0),\n",
        "            img_feat.unsqueeze(0))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz3FQOWZQwA_"
      },
      "source": [
        "import json\n",
        "from transformers.pytorch_transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#input_ids = torch.tensor(tokenizer.encode(\"[CLS] Hello, my dog is cute\")).unsqueeze(0)\n",
        "# input_ids=input_ids.to(device)\n",
        "vinvl_det_dict = torch.load('path_to_vinvl_output')\n",
        "vinvl_det_dict.keys()\n",
        "labelmap_file = 'path_to_vinvl_labelmap'\n",
        "dataset_allmap = json.load(open(labelmap_file, 'r'))\n",
        "dataset_labelmap = {int(val): key for key, val in dataset_allmap['label_to_idx'].items()}\n",
        "img_feat = get_img_features(vinvl_det_dict)['selfie']\n",
        "input = tensorize_example(tokenizer, 'This is a person', 'hair nose face shirt ear ear man eye eye nostril head mouth collar lip eyebrow forehead eyebrow chin lip neck background', img_feat,  cls_token_at_end=False, pad_on_left=False,\n",
        "                cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
        "                sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "                mask_padding_with_zero=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajPOSOKDZ4F4"
      },
      "source": [
        "class OSCARBinaryClassification(torch.nn.Module):\n",
        "    def __init__(self, oscar, linear):\n",
        "        super(OSCARBinaryClassification, self).__init__()\n",
        "        self.l1 = oscar\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = linear\n",
        "    \n",
        "    def forward(self, input_0, input_1, input_2, input_3):\n",
        "        _, output_1= self.l1(input_ids=input_0, attention_mask = input_1, token_type_ids=input_2, img_feats=input_3)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usOngEKlaODO"
      },
      "source": [
        "linear = nn.Linear(768, 2, bias=True)\n",
        "linear.load_state_dict(torch.load('path_to_oscar_binary_classifier'))\n",
        "oscarb = OSCARBinaryClassification(model.to('cpu'), linear)\n",
        "oscarb.to(device)\n",
        "model.to(device)\n",
        "model(input[0].to(device).long(), input[1][:, :128].to(device).long(), input[2].to(device).long(), input[3].to(device).long())[0].to('cpu')\n",
        "oscarb(input[0].to(device), input[1][:, :128].to(device), input[2].to(device), input[3].to(device))[0].to('cpu')\n",
        "model(input_ids=input[0].to(device), attention_mask = input[1].to(device), token_type_ids=input[2].to(device), img_feats=input[3].to(device))[1].to('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-NlI2cdPqJR"
      },
      "source": [
        "##Output OSCAR Tensors for Trainingï¼Œ Inputs are from VinVl "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prx_ENovaG9A"
      },
      "source": [
        "%cd /content/drive/MyDrive/Oscar\n",
        "\n",
        "!python setup.py build develop\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18S5a4_BSq4B"
      },
      "source": [
        "import torch\n",
        "from oscar.modeling.modeling_bert import BertImgModel\n",
        "from transformers.pytorch_transformers import BertConfig\n",
        "oscar_state_dict = torch.load('path_to_finetuned_oscar_weights')\n",
        "config = BertConfig.from_json_file('path_to_oscar_configuration')\n",
        "model = BertImgModel(config)\n",
        "model.load_state_dict(oscar_state_dict)\n",
        "model.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_feuS6Z9S0bZ"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc3rS3thWAnO"
      },
      "source": [
        "import json\n",
        "from transformers.pytorch_transformers import BertTokenizer\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "device = xm.xla_device()\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "labelmap_file = 'path_to_vinvl_labelmap_json'\n",
        "dataset_allmap = json.load(open(labelmap_file, 'r'))\n",
        "dataset_labelmap = {int(val): key for key, val in dataset_allmap['label_to_idx'].items()}\n",
        "\n",
        "def get_objects(vinvl_entry, dataset_labelmap):\n",
        "  #get objects from each vinvl prediction, and concatenate them to string\n",
        "  object_string = ''\n",
        "  for i in vinvl_entry['labels']:\n",
        "    box_feature = i\n",
        "    object_string+=dataset_labelmap[i.item()]+' '\n",
        "  return object_string\n",
        "\n",
        "def get_img_features(vinvl_entry):\n",
        "  feature_list = []\n",
        "  for a, b in zip(vinvl_entry['box_features'], vinvl_entry['spatial_features']):\n",
        "    object_feature = torch.cat((a, b))\n",
        "    feature_list.append(object_feature)\n",
        "  return torch.stack(feature_list)\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "  # This is a simple heuristic which will always truncate the longer sequence\n",
        "  # one token at a time. This makes more sense than truncating an equal percent\n",
        "  # of tokens from each, since if one sequence is very short then each token\n",
        "  # that's truncated likely contains more information than a longer sequence.\n",
        "  while True:\n",
        "      total_length = len(tokens_a) + len(tokens_b)\n",
        "      if total_length <= max_length:\n",
        "          break\n",
        "      if len(tokens_a) > len(tokens_b):\n",
        "          tokens_a.pop()\n",
        "      else:\n",
        "          tokens_b.pop()\n",
        "\n",
        "#args.max_seq_length = 128\n",
        "def tensorize_example(tokenizer, text_a, text_b, img_feat,  cls_token_at_end=False, pad_on_left=False,\n",
        "                cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
        "                sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "                mask_padding_with_zero=True):\n",
        "    max_seq_length = 128\n",
        "    max_img_seq_length = 40\n",
        "    tokens_a = tokenizer.tokenize(text_a)\n",
        "\n",
        "    tokens_b = None\n",
        "    if text_b:\n",
        "        tokens_b = tokenizer.tokenize(text_b)\n",
        "        # Modifies `tokens_a` and `tokens_b` in place so that the total length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "    else:\n",
        "        # Account for [CLS] and     [SEP] with \"- 2\"\n",
        "        if len(tokens_a) > max_seq_length - 2:\n",
        "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "    tokens = tokens_a + [sep_token]\n",
        "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "    if tokens_b:\n",
        "        tokens += tokens_b + [sep_token]\n",
        "        segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "    if cls_token_at_end:\n",
        "        tokens = tokens + [cls_token]\n",
        "        segment_ids = segment_ids + [cls_token_segment_id]\n",
        "    else:\n",
        "        tokens = [cls_token] + tokens\n",
        "        segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    padding_length = max_seq_length - len(input_ids)\n",
        "    if pad_on_left:\n",
        "        input_ids = ([pad_token] * padding_length) + input_ids\n",
        "        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
        "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
        "    else:\n",
        "        input_ids = input_ids + ([pad_token] * padding_length)\n",
        "        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "\n",
        "    # image features\n",
        "    \n",
        "    if img_feat:\n",
        "      if img_feat.shape[0] > max_img_seq_length:\n",
        "          img_feat = img_feat[0: max_img_seq_length, ]\n",
        "          if max_img_seq_length > 0:\n",
        "              input_mask = input_mask + [1 if mask_padding_with_zero else 0] * img_feat.shape[0]\n",
        "              segment_ids += [sequence_b_segment_id] * img_feat.shape[0]\n",
        "      else:\n",
        "          if max_img_seq_length > 0:\n",
        "              input_mask = input_mask + [1 if mask_padding_with_zero else 0] * img_feat.shape[0]\n",
        "              segment_ids = segment_ids + [sequence_b_segment_id] * img_feat.shape[0]\n",
        "          padding_matrix = torch.zeros((max_img_seq_length - img_feat.shape[0], img_feat.shape[1]))\n",
        "          img_feat = torch.cat((img_feat, padding_matrix), 0)\n",
        "          if max_img_seq_length > 0:\n",
        "              input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_matrix.shape[0])\n",
        "              segment_ids = segment_ids + [pad_token_segment_id] * padding_matrix.shape[0]\n",
        "\n",
        " \n",
        "\n",
        "      return (torch.tensor(input_ids, dtype=torch.long).unsqueeze(0),\n",
        "            torch.tensor(input_mask, dtype=torch.long).unsqueeze(0),\n",
        "            torch.tensor(segment_ids, dtype=torch.long)[0:128].unsqueeze(0),\n",
        "            img_feat.unsqueeze(0))\n",
        "    else:\n",
        "      return (torch.tensor(input_ids, dtype=torch.long).unsqueeze(0),\n",
        "            torch.tensor(input_mask, dtype=torch.long).unsqueeze(0),\n",
        "            torch.tensor(segment_ids, dtype=torch.long)[0:128].unsqueeze(0),\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_oscar_output(vinvl_output_file_path, greater_save_directory, dataset_labelmap, tokenizer, device):\n",
        "  vinvl_outputs_dict = torch.load(vinvl_output_file_path)\n",
        "  oscar_outputs_dict = {}\n",
        "  n = 0\n",
        "  for dict_id in list(vinvl_outputs_dict.keys()):\n",
        "    if n%100 == 0:\n",
        "      print(n)\n",
        "\n",
        "    n+=1\n",
        "    vinvl_entry = vinvl_outputs_dict[dict_id]\n",
        "    text_a = vinvl_entry['caption']\n",
        "    text_b = get_objects(vinvl_entry, dataset_labelmap)\n",
        "    img_feats = get_img_features(vinvl_entry)\n",
        "    input = tensorize_example(tokenizer, text_a, text_b, img_feats,\n",
        "                             cls_token_at_end=False, pad_on_left=False,\n",
        "                cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
        "                sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "                mask_padding_with_zero=True)\n",
        "    outputs = model(input_ids=input[0].to(device), attention_mask = input[1].to(device), token_type_ids=input[2].to(device), img_feats=input[3].to(device))\n",
        "    output1 = outputs[0].to('cpu').squeeze(0)[0]\n",
        "    output2 = outputs[1].to('cpu')\n",
        "    outputs = {}\n",
        "    outputs['hateful'] = vinvl_entry['hateful']\n",
        "    outputs['vector1'] = output1\n",
        "    outputs['vector2'] = output2\n",
        "    oscar_outputs_dict[dict_id] = outputs\n",
        "  num = [int(i) for i in list(vinvl_output_file_path) if i.isdigit()]\n",
        "  num = ''.join([str(elem) for elem in num])\n",
        "  output_path = os.path.join(greater_save_directory, 'oscar_tensors_'+num+'.pth')\n",
        "  torch.save(oscar_outputs_dict, output_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iInjkOYjboUz"
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "from transformers.pytorch_transformers import BertTokenizer\n",
        "\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "device = xm.xla_device()\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "greater_save_directory = 'path_to_save_test_set_output_finetuned_oscar_tensor'\n",
        "labelmap_file = 'path_to_vinvl_labelmap_json'\n",
        "dataset_allmap = json.load(open(labelmap_file, 'r'))\n",
        "dataset_labelmap = {int(val): key for key, val in dataset_allmap['label_to_idx'].items()}\n",
        "\n",
        "greater_vinvl_path = 'path_to_test_vinvl_tensors'\n",
        "for vinvl_names in os.listdir(greater_vinvl_path):\n",
        "  vinvl_output_file_path = os.path.join(greater_vinvl_path, vinvl_names)\n",
        "  print('***************************************')\n",
        "  print(vinvl_output_file_path)\n",
        "  save_oscar_output(vinvl_output_file_path, greater_save_directory, dataset_labelmap, tokenizer, device)\n",
        "  print()\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}