{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Training_OSCAR_Linear_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfZPWhpjF81s"
      },
      "source": [
        "#***Before Starting Runtime***\n",
        "##Need to Modify `modeling_bert.py` in `oscar` repo in order to accomodate for training batch:\n",
        "##Change line 269 to\n",
        "\n",
        "\n",
        "```\n",
        "if embedding_output.dim()==4:\n",
        "            embedding_output = torch.cat((embedding_output, img_embedding_output), 2)\n",
        "else:\n",
        "            embedding_output = torch.cat((embedding_output, img_embedding_output), 1)\n",
        "\n",
        "if embedding_output.dim()==4:\n",
        "          embedding_output = embedding_output.squeeze(1)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrnK-lFTc7FL"
      },
      "source": [
        "# %cd /content/drive/MyDrive/Oscar\n",
        "\n",
        "# !python setup.py build develop\n",
        "\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import time\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# import torch_xla.debug.metrics as met\n",
        "# import torch_xla.distributed.parallel_loader as pl\n",
        "# import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "# import torch_xla.utils.utils as xu\n",
        "\n",
        "#IF import torch_xla is unsecessul, try the following versions of pytorch to resolve version incompatiblness\n",
        "!pip install torch==1.7 \n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl\n",
        "%cd /content/drive/MyDrive/Oscar\n",
        "\n",
        "!python setup.py build develop\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZNwZjgSA5Dq"
      },
      "source": [
        "##First Train Linear Classifier on A Single TPU Core"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mslhDP6O8dz"
      },
      "source": [
        "import os\n",
        "def combine_oscar_pths(oscar_pths_directory_path):\n",
        "  pths_list = {}\n",
        "  for pth_names in os.listdir(oscar_pths_directory_path):\n",
        "    pth_file_path = os.path.join(oscar_pths_directory_path, pth_names)\n",
        "    oscar_pth = torch.load(pth_file_path)\n",
        "    for entry_name in list(oscar_pth.keys()):\n",
        "      pths_list[entry_name] = oscar_pth[entry_name]\n",
        "  return pths_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yckus93qD8N4"
      },
      "source": [
        "oscar_pths_directory_path = 'path_to_vinvl_outputs_from_training_set'\n",
        "train_oscar_data = combine_oscar_pths(oscar_pths_directory_path)\n",
        "oscar_pths_directory_path = 'path_to_vinvl_outputs_from_test_set'\n",
        "eval_oscar_data = combine_oscar_pths(oscar_pths_directory_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVogds2rZnXa"
      },
      "source": [
        "train_oscar_data = 0\n",
        "eval_oscar_data = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pE-YYgvCiAd"
      },
      "source": [
        "import json\n",
        "import torch\n",
        "import os\n",
        "import os.path as op\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "class LinearClassificationDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, oscar_pth):\n",
        "        self.all_hatefuls = self.get_all_hateful(oscar_pth)\n",
        "        self.all_oscar_features = self.get_all_oscar_features(oscar_pth)\n",
        "        \n",
        "\n",
        "\n",
        "    #get a dictionary of 2-D image feature tensors (n, 2054), n is the number of objects in an image\n",
        "    def get_all_oscar_features(self, oscar_prediction_dictionary):\n",
        "      features_list = []\n",
        "      for entry_name in list(oscar_prediction_dictionary.keys()):\n",
        "        s = oscar_prediction_dictionary[entry_name]['vector2'].squeeze(0)\n",
        "        s = s.detach().numpy()\n",
        "        s = torch.tensor(s, requires_grad=False) #This is to prevent\n",
        "                                                #RuntimeError: index_select(): functions with out=… \n",
        "                                                #arguments don’t support automatic differentiation,\n",
        "                                                # but one of the arguments requires grad.\n",
        "        features_list.append(s)\n",
        "      return features_list\n",
        "\n",
        "\n",
        "\n",
        "    def get_all_hateful(self, oscar_prediction_dictionary):\n",
        "      hateful_list = []\n",
        "      for img_name in list(oscar_prediction_dictionary.keys()):\n",
        "        hateful = oscar_prediction_dictionary[img_name]['hateful']\n",
        "        hateful_list.append(hateful)\n",
        "\n",
        "      return hateful_list\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_hatefuls)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input = self.all_oscar_features[index]\n",
        "        hateful = self.all_hatefuls[index]\n",
        "        hateful = [1, 0] if hateful ==1 else [0, 1]\n",
        "        hateful = torch.tensor(hateful, dtype = torch.float32, requires_grad=False)\n",
        "        \n",
        "\n",
        "\n",
        "        return {\n",
        "            'input': input,\n",
        "            'hateful': hateful\n",
        "        }\n",
        "\n",
        "#dataset1 = HatefulImagesDataset(vinvl_tensors_0, tokenizer, labelmap_file)\n",
        "# len(dataset1.all_img_features)\n",
        "# inx = dataset1[0]\n",
        "# inx['hateful'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncF0cIxmJtUz"
      },
      "source": [
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    with torch.no_grad():\n",
        "        for bi, data in enumerate(data_loader):\n",
        "            input = data['input']\n",
        "            targets = data['hateful']\n",
        "\n",
        "            input = input.to(device, dtype=torch.long)\n",
        "            targets = targets.to(device, dtype=torch.float)\n",
        "            outputs = model(\n",
        "                input\n",
        "            )\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGm9Xo80g_gi"
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device,epoch,num_steps):\n",
        "    model.train()\n",
        "\n",
        "    for bi, data in enumerate(data_loader):\n",
        "        print(bi)\n",
        "        input = data['input']\n",
        "        targets = data['hateful']\n",
        "        input = torch.tensor(input.detach().numpy(), requires_grad=True)\n",
        "        targets = torch.tensor(targets.detach().numpy(), requires_grad=True)\n",
        "        input = input.to(device, dtype=torch.float)\n",
        "        targets = targets.to(device, dtype=torch.float)\n",
        "        outputs = model(\n",
        "            input\n",
        "        ) \n",
        "        print('output')\n",
        "\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        print('loss')\n",
        "        xm.optimizer_step(optimizer,barrier=True)\n",
        "        print('stepping')\n",
        "\n",
        " \n",
        "        if (bi+1) % 50 == 0:\n",
        "            print('Epoch [{}/{}], bi[{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, 1, bi+1,num_steps, loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1JvewaxAz39"
      },
      "source": [
        "def eval_fn(data_loader, model, device):\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    with torch.no_grad():\n",
        "        for bi, data in enumerate(data_loader):\n",
        "            input = data['input']\n",
        "            targets = data['hateful']\n",
        "            input = torch.tensor(input.detach().numpy(), requires_grad=True)\n",
        "            targets = torch.tensor(targets.detach().numpy(), requires_grad=True)\n",
        "            input = input.to(device, dtype=torch.float)\n",
        "            targets = targets.to(device, dtype=torch.float)\n",
        "            outputs = model(\n",
        "                input\n",
        "            )\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8QxsDrdf9ud"
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "      return nn.BCEWithLogitsLoss()(outputs, targets)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MO-OZLkBVDy"
      },
      "source": [
        "def run():\n",
        "    TRAIN_BATCH_SIZE = 32\n",
        "    EPOCHS = 1\n",
        "    MODEL_PATH = '/content/drive/MyDrive/2021_Hate_Speech_Detection/model_pth/linear.pth'\n",
        "    train_dataset = LinearClassificationDataset(train_oscar_data)\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "          train_dataset,\n",
        "          num_replicas=xm.xrt_world_size(),\n",
        "          rank=xm.get_ordinal(),\n",
        "          shuffle=True)\n",
        "\n",
        "    train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=32,\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "\n",
        "    eval_dataset = LinearClassificationDataset(eval_oscar_data)\n",
        "\n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "          eval_dataset,\n",
        "          num_replicas=xm.xrt_world_size(),\n",
        "          rank=xm.get_ordinal(),\n",
        "          shuffle=False)\n",
        "\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=64,\n",
        "        sampler=valid_sampler,\n",
        "        drop_last=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    device = xm.xla_device()\n",
        "    model = torch.nn.Linear(768, 2, bias= True)\n",
        "    model.to(device)\n",
        "\n",
        "    \n",
        "    \n",
        "    lr = 1e-5 * xm.xrt_world_size()    #You can or cannot make this change , it will work if not multiplied with xm.xrt_world_size()\n",
        "\n",
        "    num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    best_accuracy = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        print('this is epch: '+ str(epoch+1))\n",
        "        train_fn(train_data_loader, model, optimizer, device, epoch=epoch, num_steps=num_train_steps)\n",
        "        \n",
        "        outputs, targets = eval_fn(valid_data_loader, model, device)\n",
        "        outputs = [[1, 0] if out_0[0] >= out_0[1] else [0, 1] for out_0 in outputs]\n",
        "        outputs = [1 if out_0 == [1, 0] else 0 for out_0 in outputs]\n",
        "        outputs = np.array(outputs)\n",
        "        targets = [1 if targ == [1, 0] else 0 for targ in targets]\n",
        "        targets = np.array(targets)\n",
        "\n",
        "        accuracy = metrics.roc_auc_score(targets, outputs)\n",
        "        acc = metrics.accuracy_score(targets, outputs)\n",
        "        print(f\"AUC_SCORE = {accuracy}\")\n",
        "        print(f\"acc = {acc}\")\n",
        "\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            xm.save(model.state_dict(), MODEL_PATH)\n",
        "            best_accuracy = accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EkQDShWBZhK"
      },
      "source": [
        "##Fine-Tune OSCAR-base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyUaKFNo1Z0H"
      },
      "source": [
        "# %cd /content/drive/MyDrive/Oscar\n",
        "\n",
        "# !python setup.py build develop\n",
        "\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import time\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# import torch_xla.debug.metrics as met\n",
        "# import torch_xla.distributed.parallel_loader as pl\n",
        "# import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "# import torch_xla.utils.utils as xu\n",
        "\n",
        "#IF import torch_xla is unsecessul, try the following versions of pytorch to resolve version incompatiblness\n",
        "!pip install torch==1.7 \n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl\n",
        "%cd /content/drive/MyDrive/Oscar\n",
        "\n",
        "!python setup.py build develop\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5_xaJsnGhmz"
      },
      "source": [
        "# def get_labelmap( labelmap_path):\n",
        "#   dataset_allmap = json.load(open(labelmap_path, 'r'))\n",
        "#   dataset_labelmap = {int(val): key for key, val in dataset_allmap['label_to_idx'].items()}\n",
        "#   return dataset_labelmap\n",
        "\n",
        "# #return dictionary of image names and corresponidng objects names strings\n",
        "# def get_all_objects( vinvl_prediction_dictionary, dataset_labelmap):\n",
        "#   #get objects from each vinvl prediction, and concatenate them to string\n",
        "#   objects_strings=[]\n",
        "#   for img_name in list(vinvl_prediction_dictionary.keys()):\n",
        "#     object_string = ''\n",
        "#     for i in vinvl_prediction_dictionary[img_name]['labels']:\n",
        "#       box_feature = i\n",
        "#       object_string+=dataset_labelmap[i.item()]+' '\n",
        "\n",
        "#     objects_strings.append(object_string)\n",
        "#   return objects_strings\n",
        "\n",
        "# import os\n",
        "# def combine_vinvl_pths(vinvl_pths_directory_path):\n",
        "#   pths_list = {}\n",
        "#   for pth_names in os.listdir(vinvl_pths_directory_path):\n",
        "#     pth_file_path = os.path.join(vinvl_pths_directory_path, pth_names)\n",
        "#     vinvl_pth = torch.load(pth_file_path)\n",
        "#     for entry_name in list(vinvl_pth.keys()):\n",
        "#       pths_list[entry_name] = vinvl_pth[entry_name]\n",
        "#   return pths_list\n",
        "\n",
        "\n",
        "# vinvl_pths_directory_path = 'path_to_vinvl_output_from_training_set'\n",
        "# train_vinvl_data = combine_vinvl_pths(vinvl_pths_directory_path)\n",
        "\n",
        "# label = get_labelmap(labelmap_file_path)\n",
        "\n",
        "# train_labels = get_all_objects(train_vinvl_data, label)\n",
        "\n",
        "# n = 0\n",
        "# for i in train_labels:\n",
        "#   print(f\"{n}: {i}\")\n",
        "#   n+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx2L11s-Cfm8"
      },
      "source": [
        "import json\n",
        "import torch\n",
        "import os\n",
        "import os.path as op\n",
        "from collections import OrderedDict\n",
        "from transformers.pytorch_transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "labelmap_file_path = 'path_to_vinvl_label_map_json'\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "class HatefulImagesDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, vinvl_pth, tokenizer, labelmap_path):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.labelmap = self.get_labelmap(labelmap_path)\n",
        "        self.all_objects_strings = self.get_all_objects(vinvl_pth, self.labelmap)\n",
        "        self.all_captions = self.get_all_captions(vinvl_pth)\n",
        "        self.all_hatefuls = self.get_all_hateful(vinvl_pth)\n",
        "        self.all_img_features = self.get_all_img_features(vinvl_pth)\n",
        "        \n",
        "\n",
        "        \n",
        "    def get_labelmap(self, labelmap_path):\n",
        "      dataset_allmap = json.load(open(labelmap_path, 'r'))\n",
        "      dataset_labelmap = {int(val): key for key, val in dataset_allmap['label_to_idx'].items()}\n",
        "      return dataset_labelmap\n",
        "      \n",
        "\n",
        "    #return dictionary of image names and corresponidng objects names strings\n",
        "    def get_all_objects(self, vinvl_prediction_dictionary, dataset_labelmap):\n",
        "      #get objects from each vinvl prediction, and concatenate them to string\n",
        "      objects_strings=[]\n",
        "      for img_name in list(vinvl_prediction_dictionary.keys()):\n",
        "        object_string = ''\n",
        "        for i in vinvl_prediction_dictionary[img_name]['labels']:\n",
        "          box_feature = i\n",
        "          object_string+=dataset_labelmap[i.item()]+' '\n",
        "\n",
        "        objects_strings.append(object_string)\n",
        "      return objects_strings\n",
        "\n",
        "    #get a dictionary of 2-D image feature tensors (n, 2054), n is the number of objects in an image\n",
        "    def get_all_img_features(self, vinvl_prediction_dictionary):\n",
        "      feature_dict = []\n",
        "      for img_name in list(vinvl_prediction_dictionary.keys()):\n",
        "        feature_list = []\n",
        "        for a, b in zip(vinvl_prediction_dictionary[img_name]['box_features'], vinvl_prediction_dictionary[img_name]['spatial_features']):\n",
        "          object_feature = torch.cat((a, b))\n",
        "          object_feature = torch.tensor(object_feature.detach().numpy(), requires_grad=False)\n",
        "          feature_list.append(object_feature)\n",
        "        \n",
        "        feature_dict.append(torch.stack(feature_list))\n",
        "\n",
        "      return feature_dict\n",
        "\n",
        "    def get_all_captions(self, vinvl_pth):\n",
        "      caption_list = []\n",
        "      for img_name in list(vinvl_pth.keys()):\n",
        "        caption = vinvl_pth[img_name]['caption']\n",
        "        caption_list.append(caption)\n",
        "\n",
        "      return caption_list\n",
        "\n",
        "    def get_all_hateful(self, vinvl_pth):\n",
        "      hateful_list = []\n",
        "      for img_name in list(vinvl_pth.keys()):\n",
        "        hateful = vinvl_pth[img_name]['hateful']\n",
        "        hateful_list.append(hateful)\n",
        "\n",
        "      return hateful_list\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_hatefuls)\n",
        "\n",
        "\n",
        "\n",
        "    def truncate_seq_pair(self, tokens_a, tokens_b, max_length):\n",
        "        \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "        # This is a simple heuristic which will always truncate the longer sequence\n",
        "        # one token at a time. This makes more sense than truncating an equal percent\n",
        "        # of tokens from each, since if one sequence is very short then each token\n",
        "        # that's truncated likely contains more information than a longer sequence.\n",
        "        while True:\n",
        "            total_length = len(tokens_a) + len(tokens_b)\n",
        "            if total_length <= max_length:\n",
        "                break\n",
        "            if len(tokens_a) > len(tokens_b):\n",
        "                tokens_a.pop()\n",
        "            else:\n",
        "                tokens_b.pop()\n",
        "\n",
        "        return tokens_a, tokens_b\n",
        "\n",
        "    #args.max_seq_length = 128\n",
        "    def tensorize_example(self, text_a, text_b, img_feat, cls_token_at_end=False, pad_on_left=False,\n",
        "                    cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
        "                    sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                    cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "                    mask_padding_with_zero=True):\n",
        "        tokenizer = self.tokenizer\n",
        "        max_seq_length = 128\n",
        "        max_img_seq_length = 40\n",
        "        tokens_a = tokenizer.tokenize(text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if text_b:\n",
        "            tokens_b = tokenizer.tokenize(text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            tokens_a, tokens_b = self.truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and     [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        tokens = tokens_a + [sep_token]\n",
        "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [sep_token]\n",
        "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "        if cls_token_at_end:\n",
        "            tokens = tokens + [cls_token]\n",
        "            segment_ids = segment_ids + [cls_token_segment_id]\n",
        "        else:\n",
        "            tokens = [cls_token] + tokens\n",
        "            segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        if pad_on_left:\n",
        "            input_ids = ([pad_token] * padding_length) + input_ids\n",
        "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
        "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
        "        else:\n",
        "            input_ids = input_ids + ([pad_token] * padding_length)\n",
        "            input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "            segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "\n",
        "        # image features\n",
        "        \n",
        "\n",
        "        if img_feat.shape[0] > max_img_seq_length:\n",
        "            img_feat = img_feat[0: max_img_seq_length, ]\n",
        "            if max_img_seq_length > 0:\n",
        "                input_mask = input_mask + [1 if mask_padding_with_zero else 0] * img_feat.shape[0]\n",
        "                segment_ids += [sequence_b_segment_id] * img_feat.shape[0]\n",
        "        else:\n",
        "            if max_img_seq_length > 0:\n",
        "                input_mask = input_mask + [1 if mask_padding_with_zero else 0] * img_feat.shape[0]\n",
        "                segment_ids = segment_ids + [sequence_b_segment_id] * img_feat.shape[0]\n",
        "            padding_matrix = torch.zeros((max_img_seq_length - img_feat.shape[0], img_feat.shape[1]))\n",
        "            img_feat = torch.cat((img_feat, padding_matrix), 0)\n",
        "            if max_img_seq_length > 0:\n",
        "                input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_matrix.shape[0])\n",
        "                segment_ids = segment_ids + [pad_token_segment_id] * padding_matrix.shape[0]\n",
        "\n",
        "    \n",
        "\n",
        "        return (torch.tensor(input_ids).unsqueeze(0),\n",
        "                torch.tensor(input_mask)[0:128].unsqueeze(0),\n",
        "                torch.tensor(segment_ids)[0:128].unsqueeze(0),\n",
        "                img_feat.unsqueeze(0))\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        caption = self.all_captions[index]\n",
        "        img_feature = self.all_img_features[index]\n",
        "        hateful = self.all_hatefuls[index]\n",
        "        hateful = [1, 0] if hateful ==1 else [0, 1]\n",
        "        hateful = torch.tensor(hateful, dtype = torch.float32).unsqueeze(0)\n",
        "        objects_string = self.all_objects_strings[index]\n",
        "        inputs = self.tensorize_example(caption, objects_string, img_feature)\n",
        "        \n",
        "\n",
        "\n",
        "        return {\n",
        "            'oscar_input': inputs,\n",
        "            'hateful': hateful\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class OscarInputDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, input_ids_tensors, attention_mask_tensors, token_type_ids_tensors, img_features_tensors, hateful_tensors):\n",
        "        self.input0 = input_ids_tensors\n",
        "        self.input1 = attention_mask_tensors\n",
        "        self.input2 = token_type_ids_tensors\n",
        "        self.input3 = img_features_tensors\n",
        "        self.hateful = hateful_tensors\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.hateful.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        input0 = self.input0[index]\n",
        "        input1 = self.input1[index]\n",
        "        input2 = self.input2[index]\n",
        "        input3 = self.input3[index]\n",
        "        hateful = self.hateful[index]\n",
        "        \n",
        "\n",
        "\n",
        "        return {\n",
        "            'oscar_input': (input0, input1, input2, input3),\n",
        "            'hateful': hateful\n",
        "        }\n",
        "\n",
        "#dataset1 = HatefulImagesDataset(vinvl_tensors_0, tokenizer, labelmap_file)\n",
        "# len(dataset1.all_img_features)\n",
        "# inx = dataset1[0]\n",
        "# inx['hateful']\n",
        "#dataset1 = HatefulImagesDataset(vinvl_tensors_0, tokenizer, labelmap_file)\n",
        "# len(dataset1.all_img_features)\n",
        "# inx = dataset1[0]\n",
        "# inx['hateful']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCQXvmV-rSKh"
      },
      "source": [
        "import os\n",
        "def combine_vinvl_pths(vinvl_pths_directory_path):\n",
        "  pths_list = {}\n",
        "  for pth_names in os.listdir(vinvl_pths_directory_path):\n",
        "    pth_file_path = os.path.join(vinvl_pths_directory_path, pth_names)\n",
        "    vinvl_pth = torch.load(pth_file_path)\n",
        "    for entry_name in list(vinvl_pth.keys()):\n",
        "      pths_list[entry_name] = vinvl_pth[entry_name]\n",
        "  return pths_list\n",
        "\n",
        "\n",
        "vinvl_pths_directory_path = 'path_to_train_vinvl_tensors'\n",
        "train_vinvl_data = combine_vinvl_pths(vinvl_pths_directory_path)\n",
        "vinvl_pths_directory_path = 'path_to_test_vinvl_tensors'\n",
        "eval_vinvl_data = combine_vinvl_pths(vinvl_pths_directory_path)\n",
        "\n",
        "\n",
        "from oscar.modeling.modeling_bert import BertImgModel\n",
        "from transformers.pytorch_transformers import BertConfig\n",
        "# oscar_state_dict = torch.load('/content/drive/MyDrive/2021_Hate_Speech_Detection/model_pth/oscar_base.pth')\n",
        "config = BertConfig.from_json_file('/content/drive/MyDrive/2021_Hate_Speech_Detection/model_pth/oscar_config.json')\n",
        "# oscar0 = BertImgModel(config)\n",
        "# oscar0.load_state_dict(oscar_state_dict)\n",
        "\n",
        "\n",
        "\n",
        "class OSCARBinaryClassification(torch.nn.Module):\n",
        "    def __init__(self, oscar, linear):\n",
        "        super(OSCARBinaryClassification, self).__init__()\n",
        "        self.l1 = oscar\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = linear\n",
        "    \n",
        "    def forward(self, input_0, input_1, input_2, input_3):\n",
        "        _, output_1= self.l1(input_ids=input_0, attention_mask = input_1, token_type_ids=input_2, img_feats=input_3)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n",
        "\n",
        "\n",
        "# oscar_state_dict = 0\n",
        "# linear = nn.Linear(768, 2, bias=True)\n",
        "# linear.load_state_dict(torch.load('path_to_trained_linear_classifier_for_oscar'))\n",
        "# model = OSCARBinaryClassification(oscar0, linear)\n",
        "# oscar0 =0\n",
        "# linear = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrvqFce0Q4L6"
      },
      "source": [
        "oscar_dataset =  HatefulImagesDataset(train_vinvl_data, tokenizer, labelmap_file_path)\n",
        "# eval_oscar_dataset = HatefulImagesDataset(eval_vinvl_data, tokenizer, labelmap_file_path)\n",
        "train_vinvl_data = 0\n",
        "# eval_vinvl_data = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0ndb_UeVoIj"
      },
      "source": [
        "# #Prepare tpu training dataset inputs\n",
        "train_input_ids_tensors = []\n",
        "train_attention_mask_tensors = []\n",
        "train_token_type_ids_tensors = []\n",
        "train_img_features_tensors = []\n",
        "train_hateful_tensors = []\n",
        "for i in range(0, 6500):\n",
        "  data = oscar_dataset[i]\n",
        "  train_hateful_tensors.append(data['hateful'])\n",
        "  train_input_ids_tensors.append(data['oscar_input'][0])\n",
        "  train_attention_mask_tensors.append(data['oscar_input'][1])\n",
        "  train_token_type_ids_tensors.append(data['oscar_input'][2])\n",
        "  train_img_features_tensors.append(data['oscar_input'][3])\n",
        "\n",
        "\n",
        "train_input_ids_tensors = torch.stack(train_input_ids_tensors)\n",
        "train_attention_mask_tensors = torch.stack(train_attention_mask_tensors)\n",
        "train_token_type_ids_tensors = torch.stack(train_token_type_ids_tensors)\n",
        "train_img_features_tensors = torch.stack(train_img_features_tensors)\n",
        "train_hateful_tensors = torch.stack(train_hateful_tensors)\n",
        "\n",
        "\n",
        "print(train_input_ids_tensors.shape)\n",
        "print(train_attention_mask_tensors.shape)\n",
        "print(train_token_type_ids_tensors.shape)\n",
        "print(train_img_features_tensors.shape)\n",
        "print(train_hateful_tensors.shape)\n",
        "\n",
        "\n",
        "#Prepare tpu evaluation dataset inputs\n",
        "eval_input_ids_tensors = []\n",
        "eval_attention_mask_tensors = []\n",
        "eval_token_type_ids_tensors = []\n",
        "eval_img_features_tensors = []\n",
        "eval_hateful_tensors = []\n",
        "for i in range(6500, len(oscar_dataset)):\n",
        "  data = oscar_dataset[i]\n",
        "  eval_hateful_tensors.append(data['hateful'])\n",
        "  eval_input_ids_tensors.append(data['oscar_input'][0])\n",
        "  eval_attention_mask_tensors.append(data['oscar_input'][1])\n",
        "  eval_token_type_ids_tensors.append(data['oscar_input'][2])\n",
        "  eval_img_features_tensors.append(data['oscar_input'][3])\n",
        "\n",
        "\n",
        "eval_input_ids_tensors = torch.stack(eval_input_ids_tensors)\n",
        "eval_attention_mask_tensors = torch.stack(eval_attention_mask_tensors)\n",
        "eval_token_type_ids_tensors = torch.stack(eval_token_type_ids_tensors)\n",
        "eval_img_features_tensors = torch.stack(eval_img_features_tensors)\n",
        "eval_hateful_tensors = torch.stack(eval_hateful_tensors)\n",
        "\n",
        "print(eval_input_ids_tensors.shape)\n",
        "print(eval_attention_mask_tensors.shape)\n",
        "print(eval_token_type_ids_tensors.shape)\n",
        "print(eval_img_features_tensors.shape)\n",
        "print(eval_hateful_tensors.shape)\n",
        "\n",
        "oscar_dataset = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IdxgHfePddA"
      },
      "source": [
        "def loss_fn(outputs, targets):\n",
        "      return nn.BCEWithLogitsLoss()(outputs, targets)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2XE04ckBno-"
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device, epoch,num_steps):\n",
        "    model.train()\n",
        "    best_loss = 0.58\n",
        "    for bi, data in enumerate(data_loader):\n",
        "        print(bi)\n",
        "        input = data['oscar_input']\n",
        "        targets = data['hateful']\n",
        "\n",
        "        input_0 = input[0].to(device)\n",
        "        input_1 = input[1].to(device)\n",
        "        input_2 = input[2].to(device)\n",
        "        input_3 = input[3].to(device)\n",
        "        \n",
        "        targets = targets.squeeze(1).to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            input_0, input_1, input_2, input_3\n",
        "        )\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        \n",
        "        xm.optimizer_step(optimizer)\n",
        "        ###################################################################################################################\n",
        "        #-------------------------------#------------------------#----------------------------#---------------------------#\n",
        "\n",
        "        if (bi+1) % 10 == 0:\n",
        "            print('Epoch [{}/{}], bi[{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, 1, bi+1,num_steps, loss.item()))\n",
        "            if loss.item() < best_loss:\n",
        "                best_loss = loss.item()\n",
        "\n",
        "\n",
        "           \n",
        "    return best_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiQQoI7xiW6f"
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "def _run():\n",
        "    \n",
        "    train_dataset = OscarInputDataset(train_input_ids_tensors, train_attention_mask_tensors, train_token_type_ids_tensors, train_img_features_tensors, train_hateful_tensors)\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "      train_dataset,\n",
        "      num_replicas=xm.xrt_world_size(),\n",
        "      rank=xm.get_ordinal(),\n",
        "      shuffle=True)\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=FLAGS['batch_size'],\n",
        "      sampler=train_sampler,\n",
        "      num_workers=FLAGS['num_workers'],\n",
        "      drop_last=True)\n",
        "    \n",
        "\n",
        "    # eval_dataset = OscarInputDataset(eval_input_ids_tensors, eval_attention_mask_tensors, eval_token_type_ids_tensors, eval_img_features_tensors, eval_hateful_tensors)\n",
        "\n",
        "    # eval_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    #   eval_dataset,\n",
        "    #   num_replicas=xm.xrt_world_size(),\n",
        "    #   rank=xm.get_ordinal(),\n",
        "    #   shuffle=False)\n",
        "    \n",
        "    # eval_loader = torch.utils.data.DataLoader(\n",
        "    #   eval_dataset,\n",
        "    #   batch_size=64,\n",
        "    #   sampler=train_sampler,\n",
        "    #   num_workers=FLAGS['num_workers'],\n",
        "    #   drop_last=False)\n",
        "\n",
        "    device = xm.xla_device()\n",
        "    model.to(device)\n",
        "    \n",
        "\n",
        "    lr = 0.2 * 1e-5 * xm.xrt_world_size()\n",
        "\n",
        "    num_train_steps = int(len(train_dataset) / FLAGS['batch_size'] / xm.xrt_world_size() * FLAGS['num_epochs'])\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    best_loss = 0.58\n",
        "    #---------------------------------------#--------------------------------#----------------------------#-------------------------------#\n",
        "    ########################################## Change occur In this Loop #################################################################\n",
        "    for epoch in range(FLAGS['num_epochs']):\n",
        "        print('this is epch: '+ str(epoch+1))\n",
        "\n",
        "        para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "        loss = train_fn(para_loader.per_device_loader(device), model, optimizer, device, epoch=epoch, num_steps=num_train_steps)\n",
        "        print(f'training loss is {loss}')\n",
        "        \n",
        "        # para_loader = pl.ParallelLoader(eval_loader, [device])\n",
        "        # outputs, targets = eval_fn(para_loader.per_device_loader(device), model, device)\n",
        "\n",
        "        # outputs = [[1, 0] if out_0[0] >= out_0[1] else [0, 1] for out_0 in outputs]\n",
        "        # outputs = [1 if out_0 == [1, 0] else 0 for out_0 in outputs]\n",
        "        # outputs = np.array(outputs)\n",
        "        # targets = [1 if targ == [1, 0] else 0 for targ in targets]\n",
        "        # targets = np.array(targets)\n",
        "\n",
        "        # accuracy = metrics.roc_auc_score(targets, outputs)\n",
        "        # acc = metrics.accuracy_score(targets, outputs)\n",
        "        # print(f\"AUC_SCORE = {accuracy}\")\n",
        "        # print(f\"acc = {acc}\")\n",
        "\n",
        "\n",
        "        if loss < best_loss:\n",
        "            \n",
        "            best_loss = loss\n",
        "            print(f'best_loss = {best_loss} ')\n",
        "\n",
        "    xm.save(model.state_dict(), FLAGS['oscar_save_path'])\n",
        "    print('model saved')\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9boXv_mHqPUG"
      },
      "source": [
        "config = BertConfig.from_json_file('_path_to_oscar_config_json')\n",
        "oscar0 = BertImgModel(config)\n",
        "oscar0.load_state_dict(torch.load('path_to_fine_tuned_oscar_weight')\n",
        ")\n",
        "\n",
        "oscar_state_dict = 0\n",
        "linear = nn.Linear(768, 2, bias=True)\n",
        "model = OSCARBinaryClassification(oscar0, linear)\n",
        "oscar0 =0\n",
        "linear = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVR4Ps_W3tbm"
      },
      "source": [
        "def _mp_fn(rank, flags):\n",
        "    a = _run()\n",
        "    return a\n",
        "\n",
        "FLAGS = {}\n",
        "FLAGS['oscar_save_path'] = \"path_to_finetuned_oscar\"\n",
        "FLAGS['batch_size'] = 50\n",
        "FLAGS['num_workers'] = 5\n",
        "FLAGS['num_epochs'] = 100\n",
        "FLAGS['num_cores'] = 8 \n",
        "FLAGS['log_steps'] = 50\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeHYBjAbFKp0"
      },
      "source": [
        "#Train Set Evaluation\n",
        "import sklearn.metrics as me\n",
        "\n",
        "inputset = OscarInputDataset(train_input_ids_tensors, train_attention_mask_tensors, train_token_type_ids_tensors, train_img_features_tensors, train_hateful_tensors)\n",
        "\n",
        "device = xm.xla_device()\n",
        "model = OSCARBinaryClassification(BertImgModel(config), nn.Linear(768, 2, bias=True))\n",
        "model.load_state_dict(torch.load('path_to_finetuned_oscar'))\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "output_t = []\n",
        "label_t = []\n",
        "\n",
        "\n",
        "input = inputset[0::10]['oscar_input']\n",
        "\n",
        "input_0 = input[0].to(device)\n",
        "input_1 = input[1].to(device)\n",
        "input_2 = input[2].to(device)\n",
        "input_3 = input[3].to(device)\n",
        "        \n",
        "outputs = model(\n",
        "            input_0, input_1, input_2, input_3\n",
        "        ).to('cpu')\n",
        "\n",
        "\n",
        "\n",
        "for tup in outputs.detach().numpy().tolist():\n",
        "  if tup[0]>=tup[1]:\n",
        "    output_t.append(1)\n",
        "  else:\n",
        "    output_t.append(0)\n",
        "\n",
        "\n",
        "targets = inputset[0::10]['hateful'].detach().squeeze(1).numpy().tolist()\n",
        "for tup in targets:\n",
        "  if tup[0]>=tup[1]:\n",
        "    label_t.append(1)\n",
        "  else:\n",
        "    label_t.append(0)\n",
        "        \n",
        "output_t = np.array(output_t)\n",
        "label_t = np.array(label_t)\n",
        "\n",
        "print(f'traini set accuracy is {me.accuracy_score(label_t, output_t)}')\n",
        "print(f'train set auroc is {me.roc_auc_score(label_t, output_t)}')\n",
        "\n",
        "#Train Set Evaluation\n",
        "inputset = OscarInputDataset(eval_input_ids_tensors, eval_attention_mask_tensors, eval_token_type_ids_tensors, eval_img_features_tensors, eval_hateful_tensors)\n",
        "\n",
        "output_v = []\n",
        "label_v = []\n",
        "\n",
        "input = inputset[0::2]['oscar_input']\n",
        "\n",
        "input_0 = input[0].to(device)\n",
        "input_1 = input[1].to(device)\n",
        "input_2 = input[2].to(device)\n",
        "input_3 = input[3].to(device)\n",
        "        \n",
        "outputs = model(\n",
        "            input_0, input_1, input_2, input_3\n",
        "        ).to('cpu')\n",
        "\n",
        "for tup in outputs.detach().numpy().tolist():\n",
        "  if tup[0]>=tup[1]:\n",
        "    output_v.append(1)\n",
        "  else:\n",
        "    output_v.append(0)\n",
        "\n",
        "targets = inputset[0::2]['hateful'].detach().squeeze(1).numpy().tolist()\n",
        "for tup in targets:\n",
        "  if tup[0]>=tup[1]:\n",
        "    label_v.append(1)\n",
        "  else:\n",
        "    label_v.append(0)\n",
        "        \n",
        "output_v = np.array(output_v)\n",
        "label_v = np.array(label_v)\n",
        "\n",
        "print(f'evaluation set accuracy is {me.accuracy_score(label_v, output_v)}')\n",
        "print(f'evaluation set auroc is {me.roc_auc_score(label_v, output_v)}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}